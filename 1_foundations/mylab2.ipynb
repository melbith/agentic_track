{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b09b002",
   "metadata": {},
   "source": [
    "Lab 2 - Week 1 | Day 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24da4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from anthropic import Anthropic\n",
    "from IPython.display import Markdown,display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7b5e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b160d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "openrouter_api_key = os.getenv('OPENROUTER_API_KEY')\n",
    "\n",
    "if openai_api_key:\n",
    "    print(f\"OpenAI API Key exists and begins {openai_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"OpenAI API Key not set\")\n",
    "\n",
    "if openrouter_api_key:\n",
    "    print(f\"OpenRouter API Key exists and begins {openrouter_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"OpenRouter API Key not set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86fad0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "request = \"Please come up with a challenging, nuanced question that I can ask a number of LLMs to evaluate their intelligence. \"\n",
    "request += \"Answer only with the question, no explanation.\"\n",
    "messages = [{\"role\": \"user\", \"content\": request}]\n",
    "\n",
    "print(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9a0844",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai = OpenAI()\n",
    "response = openai.chat.completions.create(\n",
    "    model = 'gpt-4o-mini',\n",
    "    messages=messages\n",
    ")\n",
    "\n",
    "question = response.choices[0].message.content\n",
    "print(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f22f3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a list of competitors and their answers for comaprison\n",
    "competitors = []\n",
    "answers = []\n",
    "\n",
    "# Define the common messages in OpenAI format\n",
    "messages = [{\"role\": \"user\", \"content\":question}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb829124",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAI - The API we know well - \n",
    "\n",
    "model_name = \"gpt-4o-mini\"\n",
    "\n",
    "response = openai.chat.completions.create(model=model_name, messages=messages)\n",
    "answer = response.choices[0].message.content\n",
    "\n",
    "display(Markdown(answer))\n",
    "competitors.append(model_name)\n",
    "answers.append(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6985c165",
   "metadata": {},
   "source": [
    "Leverage OpenRouter to use all other Models referenced in the lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf1b0ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create openRouter client. This follows the standard OpenAI format with a base URL to openrouter\n",
    "openrouter_client = OpenAI(\n",
    "    base_url=\"https://openrouter.ai/api/v1\",\n",
    "    api_key=openrouter_api_key, # Replace with your actual key\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b2aa51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anthropic has a slightly different API, and requires Max Tokens as a param\n",
    "\n",
    "# claude = Anthropic()\n",
    "# response = claude.messages.create(model=model_name, messages=messages, max_tokens=1000)\n",
    "# answer = response.content[0].text\n",
    "\n",
    "# But now, we are leveraging the OpenRouter API to call Anthropic API instead\n",
    "\n",
    "claude=openrouter_client\n",
    "model_name = \"anthropic/claude-3-7-sonnet\"\n",
    "\n",
    "response = claude.chat.completions.create(\n",
    "        model=model_name, # Example: Using Anthropic model\n",
    "        messages=messages,\n",
    "        # max_tokens=1000 not required when accessed in openRouter\n",
    ")\n",
    "\n",
    "answer = response.choices[0].message.content\n",
    "display(Markdown(answer))\n",
    "\n",
    "competitors.append(model_name)\n",
    "answers.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13593ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gemini \n",
    "\n",
    "gemini = openrouter_client\n",
    "model_name = \"google/gemini-2.5-flash\"\n",
    "\n",
    "response = gemini.chat.completions.create(\n",
    "        model=model_name, \n",
    "        messages=messages,\n",
    ")\n",
    "\n",
    "answer = response.choices[0].message.content\n",
    "display(Markdown(answer))\n",
    "\n",
    "competitors.append(model_name)\n",
    "answers.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae9316e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deepseek\n",
    "\n",
    "deepseek = openrouter_client\n",
    "model_name = \"deepseek/deepseek-chat\"\n",
    "\n",
    "response = deepseek.chat.completions.create(\n",
    "        model=model_name, \n",
    "        messages=messages,\n",
    ")\n",
    "\n",
    "answer = response.choices[0].message.content\n",
    "display(Markdown(answer))\n",
    "\n",
    "competitors.append(model_name)\n",
    "answers.append(answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6651c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Groq - This is a provider of fast inference on Specialized hardware.\n",
    "# This is a platform that supports multiple models.\n",
    "# The model \"llama-3.3-70b-versatile\" is only available in Groq but I don't have a Groq key.\n",
    "# So I am using Meta's \"meta-llama/llama-3.3-70b-instruct\" model in OpenRouter as an alternative competitor.\n",
    "\n",
    "groq = openrouter_client\n",
    "model_name = \"meta-llama/llama-3.3-70b-instruct\"\n",
    "\n",
    "response = groq.chat.completions.create(\n",
    "        model=model_name, \n",
    "        messages=messages,\n",
    ")\n",
    "\n",
    "answer = response.choices[0].message.content\n",
    "display(Markdown(answer))\n",
    "\n",
    "competitors.append(model_name)\n",
    "answers.append(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f1abb3",
   "metadata": {},
   "source": [
    "I am skipping Ollama local installation for now - It's cod is available in Lab 2 notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce2c2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# So where are we?\n",
    "\n",
    "print(competitors)\n",
    "print(answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f40da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It's nice to know how to use \"zip\"\n",
    "# Print the competitors and their answers as a pair\n",
    "\n",
    "for competitor, answer in zip(competitors, answers):\n",
    "    message_pair=f\"Competitor: {competitor}\\n\\n{answer}\"\n",
    "    display(Markdown(message_pair))\n",
    "    # print(message_pair)\n",
    "    # print(f\"Competitor: {competitor}\\n\\n{answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641a2566",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print all the answers together in a single markdown block\n",
    "together = \"\"\n",
    "for index, answer in enumerate(answers):\n",
    "    together += f\"### Response from competitor {index+1}\\n\\n\"\n",
    "    together += answer\n",
    "\n",
    "display(Markdown(together))\n",
    "# print(together)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b8a453",
   "metadata": {},
   "source": [
    "Judgement Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cac2bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Judgement instruction\n",
    "\n",
    "judge_instruction = \"\"\n",
    "\n",
    "judge_instruction+=f\"\"\"You are judging a competition between {len(competitors)} competitors.\n",
    "Each model has been given this question:\n",
    "\n",
    "{question}\n",
    "\n",
    "Your job is to evaluate each response for clarity and strength of argument, and rank them in order of best to worst.\n",
    "Respond with JSON, and only JSON, with the following format:\n",
    "{{\"results\": [\"best competitor number\", \"second best competitor number\", \"third best competitor number\", ...]}}\n",
    "\n",
    "Here are the responses from each competitor:\n",
    "\n",
    "{together}\n",
    "\n",
    "Now respond with the JSON with the ranked order of the competitors, nothing else. \n",
    "Do not include markdown formatting or code blocks.\"\"\"\n",
    "\n",
    "judge_messages = [{\"role\": \"user\", \"content\": judge_instruction}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55dce6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Judgement time!\n",
    "\n",
    "openai = OpenAI()\n",
    "response = openai.chat.completions.create(\n",
    "    model=\"o3-mini\",\n",
    "    messages=judge_messages,\n",
    ")\n",
    "results = response.choices[0].message.content\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8ac58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's turn this into readable results!\n",
    "\n",
    "results_dict = json.loads(results)\n",
    "winners = results_dict[\"results\"]\n",
    "\n",
    "# print(winners)\n",
    "# print(competitors)\n",
    "\n",
    "for index, winner in enumerate(winners):\n",
    "    # print(index, winner)\n",
    "    competitor = competitors[int(winner)-1]\n",
    "    print(f\"Rank {index+1}: {competitor}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfabc8b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9337d435",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
