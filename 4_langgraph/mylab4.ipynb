{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70000f1c",
   "metadata": {},
   "source": [
    "<b>Lab 4 | Week 4 Day 4</b> Preparing for Day 5 project\n",
    "\n",
    "## The Sidekick\n",
    "\n",
    "It's Time to:\n",
    "- Introduce Structured output\n",
    "- Implement a Multi-agent workflow\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c701f6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from typing import Annotated, TypedDict, Dict, List, Any, Optional\n",
    "from IPython.display import Image, display\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "# from langchain.agents import Tool\n",
    "\n",
    "from langchain_community.agent_toolkits import PlayWrightBrowserToolkit\n",
    "from langchain_community.tools.playwright.utils import create_async_playwright_browser\n",
    "\n",
    "import gradio as gr, uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5121165",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49440e16",
   "metadata": {},
   "source": [
    "#### For structured outputs, we define a Pydantic object for the Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd02da05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a structured output that we expect to from the Evaluator LLM\n",
    "\n",
    "class EvaluatorOutput(BaseModel):\n",
    "    feedback: str = Field(description=\"Feedback on the assistant's response\")\n",
    "    success_criteria_met: bool = Field(description=\"Whether the success criteria have been met\")\n",
    "    user_input_needed: bool = Field(description=\"True if more input is needed from the user, or clarifications, or the assistant is stuck\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe1883a",
   "metadata": {},
   "source": [
    "#### For the State, we'll use TypedDict again\n",
    "\n",
    "But now we have some real information to maintain inside the State!\n",
    "\n",
    "The messages uses the reducer as usual. The others are simply values that we overwrite with any state change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2fa707f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The state class\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[List[Any], add_messages]\n",
    "    feedback_on_work: Optional[str]\n",
    "    success_criteria: str\n",
    "    success_criteria_met: bool\n",
    "    user_input_needed: bool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "84ebc7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Get the async Playwright tools\n",
    "# If you get a NotImplementedError here or later, see the Heads Up at the top of the mylab3 notebook\n",
    "\n",
    "async_browser = create_async_playwright_browser(headless=False) # create browser in headful mode\n",
    "toolkit = PlayWrightBrowserToolkit.from_browser(async_browser=async_browser)\n",
    "tools = toolkit.get_tools()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f8d5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the LLMs\n",
    "\n",
    "worker_llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "worker_llm_with_tools = worker_llm.bind_tools(tools)\n",
    "\n",
    "evaluator_llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "evaluator_llm_with_output = evaluator_llm.with_structured_output(EvaluatorOutput) \n",
    "# Note that some models do not have support for structured output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cfa1cb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The worker node\n",
    "\n",
    "def worker(state: State) -> dict[str, Any]:\n",
    "\n",
    "    system_message = f\"\"\"You are a helpful assistant that can use tools to complete tasks.\n",
    "                    You keep working on a task until either you have a question or clarification for the user, or the success criteria is met.\n",
    "                    This is the success criteria:\n",
    "                    {state['success_criteria']}\n",
    "                    You should reply either with a question for the user about this assignment, or with your final response.\n",
    "                    If you have a question for the user, you need to reply by clearly stating your question. An example might be:\n",
    "\n",
    "                    Question: please clarify whether you want a summary or a detailed answer\n",
    "\n",
    "                    If you've finished, reply with the final answer, and don't ask a question; simply reply with the answer.\n",
    "                    \"\"\"\n",
    "    # if there is a feedback from the evaluator, append that to the original system message\n",
    "    if state.get('feedback_on_work'):\n",
    "        system_message += f\"\"\"\n",
    "                    Previously you thought you completed the assignment, but your reply was rejected because the success criteria was not met.\n",
    "                    Here is the feedback on why this was rejected:\n",
    "                    {state['feedback_on_work']}\n",
    "                    With this feedback, please continue the assignment, ensuring that you meet the success criteria or have a question for the user.\"\"\"\n",
    "\n",
    "    # Add in the system message\n",
    "\n",
    "    found_system_message = False\n",
    "    messages = state['messages']\n",
    "\n",
    "    for message in messages:\n",
    "        if isinstance(message, SystemMessage):\n",
    "            message.content = system_message\n",
    "            found_system_message = True\n",
    "\n",
    "    if not found_system_message:\n",
    "        messages = [SystemMessage(content=system_message)] + messages\n",
    "\n",
    "    # Invoke the LLM with tools\n",
    "    response = worker_llm_with_tools.invoke(messages)\n",
    "\n",
    "    # Return updated state\n",
    "    return {\n",
    "        'messages' : [response],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6681f6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This router function will be used in a conditional edge to decide the way to route the control\n",
    "# based on the worker llm response\n",
    "\n",
    "def worker_router(state: State) -> str:\n",
    "    last_message = state['messages'][-1]\n",
    "\n",
    "    if hasattr(last_message, 'tool_calls') and last_message.tool_calls:\n",
    "        return 'tools'\n",
    "    else:\n",
    "        return 'evaluator'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d6d1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that will format the conversation to User -> Assistant flow;\n",
    "# inorder to feed in to the evaluator\n",
    "\n",
    "def format_conversation(messages: List[Any]) -> str:\n",
    "    conversation = 'Conversation History: \\n\\n'\n",
    "    for message in messages:\n",
    "        if isinstance(message, HumanMessage):\n",
    "            conversation += f\"User: {message.content}\\n\"\n",
    "        elif isinstance(message, AIMessage):\n",
    "            text = message.content or \"[Tool use]\"\n",
    "            conversation += f\"Assistant: {text}\\n\"\n",
    "    return conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8047a0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Evaluator function\n",
    "\n",
    "def evaluator(state: State) -> State:\n",
    "\n",
    "    last_response = state['messages'][-1].content\n",
    "\n",
    "    system_message = \"\"\"You are an evaluator that determines if a task has been completed successfully by an Assistant.\n",
    "                    Assess the Assistant's last response based on the given criteria. Respond with your feedback, \n",
    "                    and with your decision on whether the success criteria has been met, and whether more input is needed from the user.\n",
    "                    \"\"\"\n",
    "\n",
    "    user_message = f\"\"\"You are evaluating a conversation between the User and Assistant. You decide what action to take based on the last response from the Assistant.\n",
    "\n",
    "                    The entire conversation with the assistant, with the user's original request and all replies, is:\n",
    "                    {format_conversation(state['messages'])}\n",
    "\n",
    "                    The success criteria for this assignment is:\n",
    "                    {state['success_criteria']}\n",
    "\n",
    "                    And the final response from the Assistant that you are evaluating is:\n",
    "                    {last_response}\n",
    "\n",
    "                    Respond with your feedback, and decide if the success criteria is met by this response.\n",
    "                    Also, decide if more user input is required, either because the assistant has a question, needs clarification, or seems to be stuck and unable to answer without help.\n",
    "                    \"\"\"\n",
    "\n",
    "    if state[\"feedback_on_work\"]:\n",
    "        user_message += f\"Also, note that in a prior attempt from the Assistant, you provided this feedback: {state['feedback_on_work']}\\n\"\n",
    "        user_message += \"If you're seeing the Assistant repeating the same mistakes, then consider responding that user input is required.\"\n",
    "\n",
    "    evaluator_messages = [SystemMessage(content=system_message), HumanMessage(content=user_message)]\n",
    "    eval_result = evaluator_llm_with_output.invoke(evaluator_messages)\n",
    "\n",
    "    # Populate the new state with the values returned in the Pydantic object - EvaluatorOutput\n",
    "    new_state = {\n",
    "        'messages': [{'role': 'assistant', 'content': f\"Evaluator Feedback on this answer: {eval_result.feedback}\"}],\n",
    "        'feedback_on_work': eval_result.feedback,\n",
    "        'success_criteria_met': eval_result.success_criteria_met,\n",
    "        'user_input_needed': eval_result.user_input_needed,\n",
    "    }\n",
    "\n",
    "    return new_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2738148a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another routing based on Evaluator response\n",
    "\n",
    "def route_based_on_evaluation(state: State) -> str:\n",
    "    if state['success_criteria_met'] or state['user_input_needed']:\n",
    "        return 'END' # Superstep is completed and the control passes back to the user\n",
    "    return 'worker' # control passes back to the worker for next attempt to improve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b5251174",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up Graph Builder with State\n",
    "graph_builder = StateGraph(State)\n",
    "\n",
    "# Add nodes\n",
    "graph_builder.add_node('worker', worker)\n",
    "graph_builder.add_node('tools', ToolNode(tools=tools))\n",
    "graph_builder.add_node('evaluator', evaluator)\n",
    "\n",
    "# Add edges\n",
    "graph_builder.add_conditional_edges('worker', worker_router, {'tools': 'tools', 'evaluator': 'evaluator'})\n",
    "graph_builder.add_edge('tools', 'worker')\n",
    "graph_builder.add_conditional_edges('evaluator', route_based_on_evaluation, {'worker': 'worker', 'END': END})\n",
    "graph_builder.add_edge(START, 'worker')\n",
    "\n",
    "# Compile the graph\n",
    "memory = MemorySaver()\n",
    "graph = graph_builder.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572a7179",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cfa70d1",
   "metadata": {},
   "source": [
    "#### Gradio Callback to kick off a super-step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "41f5b37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create unique thread ids for each gradio instance\n",
    "\n",
    "def make_thread_id() -> str:\n",
    "    return str(uuid.uuid4())\n",
    "\n",
    "# Gradio Callback\n",
    "async def process_message(message, success_criteria, history, thread):\n",
    "    config = {'configurable': {'thread_id':thread}}\n",
    "\n",
    "    state = {\n",
    "        'messages': message,\n",
    "        'success_criteria': success_criteria,\n",
    "        'feedback_on_work': None,\n",
    "        'success_criteria_met': False,\n",
    "        'user_input_needed': False\n",
    "    }\n",
    "    result = await graph.ainvoke(state, config=config)\n",
    "\n",
    "    user = {'role': 'user', 'content': message}\n",
    "    reply = {'role': 'assistant', 'content': result['messages'][-2].content}\n",
    "    feedback = {'role': 'assistant', 'content': result['messages'][-1].content}\n",
    "\n",
    "    return history + [user, reply, feedback]\n",
    "\n",
    "# Reset\n",
    "async def reset():\n",
    "    return '', '', None, make_thread_id()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af98abd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with gr.Blocks(theme=gr.themes.Default(primary_hue=\"emerald\")) as demo:\n",
    "\n",
    "    gr.Markdown(\"## Sidekick Personal Co-worker\")\n",
    "\n",
    "    # Make a thread\n",
    "    thread = gr.State(make_thread_id())\n",
    "    \n",
    "    with gr.Row():\n",
    "        chatbot = gr.Chatbot(label=\"Sidekick\", height=300, type=\"messages\")\n",
    "\n",
    "    with gr.Group():\n",
    "        with gr.Row():\n",
    "            message = gr.Textbox(show_label=False, placeholder=\"Your request to your sidekick\")\n",
    "        with gr.Row():\n",
    "            success_criteria = gr.Textbox(show_label=False, placeholder=\"What are your success critiera?\")\n",
    "\n",
    "    with gr.Row():\n",
    "        reset_button = gr.Button(\"Reset\", variant=\"stop\")\n",
    "        go_button = gr.Button(\"Go!\", variant=\"primary\")\n",
    "        \n",
    "    message.submit(process_message, [message, success_criteria, chatbot, thread], [chatbot])\n",
    "    success_criteria.submit(process_message, [message, success_criteria, chatbot, thread], [chatbot])\n",
    "    go_button.click(process_message, [message, success_criteria, chatbot, thread], [chatbot])\n",
    "    reset_button.click(reset, [], [message, success_criteria, chatbot, thread])\n",
    "\n",
    "    \n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30b4488",
   "metadata": {},
   "source": [
    "---\n",
    "That's a wrap for lab 4. We will move to the Sidekick improved app in Day 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae65ca2",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
