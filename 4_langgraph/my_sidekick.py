import asyncio, uuid

from dotenv import load_dotenv
from typing import Annotated, TypedDict, Dict, List, Any, Optional
from pydantic import BaseModel, Field
from datetime import datetime

from langchain_core.messages import HumanMessage, SystemMessage, AIMessage
from langchain_openai import ChatOpenAI

from langgraph.graph import StateGraph, START, END
from langgraph.graph.message import add_messages
from langgraph.prebuilt import ToolNode
from langgraph.checkpoint.memory import MemorySaver

from my_sidekick_tools import get_other_tools, get_playwright_tools


load_dotenv(override=True)

# The state class

class State(TypedDict):
    messages: Annotated[List[Any], add_messages]
    feedback_on_work: Optional[str]
    success_criteria: str
    success_criteria_met: bool
    user_input_needed: bool

# Define a structured output that we expect to from the Evaluator LLM

class EvaluatorOutput(BaseModel):
    feedback: str = Field(description="Feedback on the assistant's response")
    success_criteria_met: bool = Field(description="Whether the success criteria have been met")
    user_input_needed: bool = Field(description="True if more input is needed from the user, or clarifications, or the assistant is stuck")

# 

class Sidekick:
    def __init__(self):

        self.tools = None
        self.llm_with_tools = None # ??
        self.worker_llm_with_tools = None
        self.evaluator_llm_with_output = None
        self.browser = None
        self.playwright = None
        self.graph = None
        self.memory = MemorySaver()        
        self.sidekick_id = str(uuid.uuid4())

    async def setup(self):
        """ This async setup function is required to do the async work
            because the __init__ can't be async in Python"""
        worker_llm = ChatOpenAI(model="gpt-4o-mini")
        evaluator_llm = ChatOpenAI(model="gpt-4o-mini")

        self.tools, self.browser, self.playwright = await get_playwright_tools()
        self.tools += await get_other_tools()
        self.worker_llm_with_tools = worker_llm.bind_tools(self.tools)
        self.evaluator_llm_with_output = evaluator_llm.with_structured_output(EvaluatorOutput)

        await self.build_graph()

    def worker(self, state: State) -> dict[str, Any]:

        system_message = f"""You are a helpful assistant that can use tools to complete tasks.
                    You keep working on a task until either you have a question or clarification for the user, or the success criteria is met.
                    You have many tools to help you, including tools to browse the internet, navigating and retrieving web pages.
                    You have a tool to run python code, but note that you would need to include a print() statement if you wanted to receive output.
                    The current date and time is {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}

                    This is the success criteria:
                    {state['success_criteria']}
                    You should reply either with a question for the user about this assignment, or with your final response.
                    If you have a question for the user, you need to reply by clearly stating your question. An example might be:

                    Question: please clarify whether you want a summary or a detailed answer

                    If you've finished, reply with the final answer, and don't ask a question; simply reply with the answer.
                    """
        # if there is a feedback from the evaluator, append that to the original system message
        if state.get('feedback_on_work'):
            system_message += f"""
                    Previously you thought you completed the assignment, but your reply was rejected because the success criteria was not met.
                    Here is the feedback on why this was rejected:
                    {state['feedback_on_work']}
                    With this feedback, please continue the assignment, ensuring that you meet the success criteria or have a question for the user.
                    """

        # Add in the system message

        found_system_message = False
        messages = state['messages']

        for message in messages:
            if isinstance(message, SystemMessage):
                message.content = system_message
                found_system_message = True

        if not found_system_message:
            messages = [SystemMessage(content=system_message)] + messages

        # Invoke the LLM with tools
        response = self.worker_llm_with_tools.invoke(messages)

        # Return updated state
        return {
            'messages' : [response],
        }

    """ 
    This router function will be used in a conditional edge to decide the way to route the control based on the worker llm response
    """
    def worker_router(self, state: State) -> str:
        last_message = state['messages'][-1]

        if hasattr(last_message, 'tool_calls') and last_message.tool_calls:
            return 'tools'
        else:
            return 'evaluator'

    """ 
    Define a function that will format the conversation to User -> Assistant flow; inorder to feed in to the evaluator
    """

    def format_conversation(self, messages: List[Any]) -> str:
        conversation = 'Conversation History: \n\n'
        for message in messages:
            if isinstance(message, HumanMessage):
                conversation += f"User: {message.content}\n"
            elif isinstance(message, AIMessage):
                text = message.content or "[Tool use]"
                conversation += f"Assistant: {text}\n"
        return conversation

    # The Evaluator function

    def evaluator(self, state: State) -> State:

        last_response = state['messages'][-1].content

        system_message = """You are an evaluator that determines if a task has been completed successfully by an Assistant.
                        Assess the Assistant's last response based on the given criteria. Respond with your feedback, 
                        and with your decision on whether the success criteria has been met, and whether more input is needed from the user.
                        """

        user_message = f"""You are evaluating a conversation between the User and Assistant. You decide what action to take based on the last response from the Assistant.

                        The entire conversation with the assistant, with the user's original request and all replies, is:
                        {self.format_conversation(state['messages'])}

                        The success criteria for this assignment is:
                        {state['success_criteria']}

                        And the final response from the Assistant that you are evaluating is:
                        {last_response}

                        Respond with your feedback, and decide if the success criteria is met by this response.
                        Also, decide if more user input is required, either because the assistant has a question, needs clarification, or seems to be stuck and unable to answer without help.

                        The Assistant has access to a tool to write files. If the Assistant says they have written a file, then you can assume they have done so.
                        Overall you should give the Assistant the benefit of the doubt if they say they've done something. But you should reject if you feel that more work should go into this.
                        """

        if state['feedback_on_work']:
            user_message += f"Also, note that in a prior attempt from the Assistant, you provided this feedback: {state['feedback_on_work']}\n"
            user_message += "If you're seeing the Assistant repeating the same mistakes, then consider responding that user input is required."

        evaluator_messages = [
            SystemMessage(content=system_message),
            HumanMessage(content=user_message)
            ]
        eval_result = self.evaluator_llm_with_output.invoke(evaluator_messages)

        # Populate the new state with the values returned in the Pydantic object - EvaluatorOutput
        new_state = {
            'messages': [
                {
                    'role': 'assistant',
                    'content': f"Evaluator Feedback on this answer: {eval_result.feedback}"
                }
            ],
            'feedback_on_work': eval_result.feedback,
            'success_criteria_met': eval_result.success_criteria_met,
            'user_input_needed': eval_result.user_input_needed,
        }

        return new_state

    # Another routing based on Evaluator response

    def route_based_on_evaluation(self, state: State) -> str:
        if state['success_criteria_met'] or state['user_input_needed']:
            return 'END' # Superstep is completed and the control passes back to the user
        return 'worker' # control passes back to the worker for next attempt to improve

    
    async def build_graph(self):
        # Set up Graph Builder with State
        graph_builder = StateGraph(State)

        # Add nodes
        graph_builder.add_node('worker', self.worker)
        graph_builder.add_node('tools', ToolNode(tools=self.tools))
        graph_builder.add_node('evaluator', self.evaluator)

        # Add edges
        graph_builder.add_conditional_edges('worker', self.worker_router, {'tools': 'tools', 'evaluator': 'evaluator'})
        graph_builder.add_edge('tools', 'worker')
        graph_builder.add_conditional_edges('evaluator', self.route_based_on_evaluation, {'worker': 'worker', 'END': END})
        graph_builder.add_edge(START, 'worker')

        # Compile the graph
        self.graph = graph_builder.compile(checkpointer=self.memory)

    # Gradio Callback ??

    async def run_superstep(self, message, success_criteria, history):
        config = {'configurable': {'thread_id': self.sidekick_id}}

        state = {
            'messages': message,
            'success_criteria': success_criteria or "The answer should be clear and accurate",
            'feedback_on_work': None,
            'success_criteria_met': False,
            'user_input_needed': False
        }
        result = await self.graph.ainvoke(state, config=config)

        user = {'role': 'user', 'content': message}
        reply = {'role': 'assistant', 'content': result['messages'][-2].content}
        feedback = {'role': 'assistant', 'content': result['messages'][-1].content}

        return history + [user, reply, feedback]

    # Cleanup Playwright resources
    
    def cleanup(self):
        if self.browser:
            try:
                loop = asyncio.get_running_loop()
                loop.create_task(self.browser.close())
                if self.playwright:
                    loop.create_task(self.playwright.stop())
            except RuntimeError:
                # If no loop is running, do a direct run
                asyncio.run(self.browser.close())
                if self.playwright:
                    asyncio.run(self.playwright.stop())